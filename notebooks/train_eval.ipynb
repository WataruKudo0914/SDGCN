{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signed Graph Convolutional Network(SGCN)を用いたFraud User Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32mmaster\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../models/')\n",
    "sys.path.append('../utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from sgcn import SignedGCNTrainer, SignedGCNPredictor\n",
    "from parser_ import parameter_parser\n",
    "from utils import tab_printer, read_graph, score_printer, save_logs\n",
    "import easydict\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データセット：alpha\n"
     ]
    }
   ],
   "source": [
    "data_name = input('データセット：')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "        \"edge_path\": f'../data/input/{data_name}/{data_name}_network.csv',\n",
    "        \"features_path\":  f'../data/input/{data_name}/{data_name}_node_feature.csv',\n",
    "        \"nodes_path\": f'../data/input/{data_name}/{data_name}_gt.csv',\n",
    "        \"embedding_path\": f'../data/tmp/embedding/{data_name}_sgcn_feature.pkl', # tmp folder for cross-validation\n",
    "        \"regression_weights_path\": f'../data/tmp/weights/{data_name}_sgcn_feature.pkl',\n",
    "        \"inductive_model_path\": None, # f'../output/inductive/{data_name}_model', # or None\n",
    "        \"log_path\": f'../logs/{data_name}_logs_feature.json',\n",
    "        \"epochs\":300,\n",
    "        \"test_size\":0.33,\n",
    "        \"reduction_iterations\": 128,\n",
    "        \"reduction_dimensions\": 30,\n",
    "        \"seed\": 42,\n",
    "        \"lamb\": 0.0,\n",
    "        \"learning_rate\": 0.001,  \n",
    "        \"weight_decay\": 10e-4, \n",
    "        # \"layers\": [64, 32,16,8],\n",
    "        \"layers\": [32,16],\n",
    "        \"spectral_features\":False,\n",
    "        \"general_features\": True,  \n",
    "        \"sample_num\":None,\n",
    "        \"class_weights\":False,\n",
    "        \"node_under_sampling\":False,\n",
    "        \"hidden_residual\":False,\n",
    "        \"eval_freq\":1,\n",
    "        \"subgraph_training\":False,\n",
    "        \"l1_lambda\":0.1,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "args = easydict.EasyDict({\n",
    "        \"edge_path\": f'../input/{data_name}/{data_name}_network.csv',\n",
    "        \"features_path\":  f'../input/{data_name}/{data_name}_node_feature.csv',\n",
    "        \"nodes_path\": f'../input/{data_name}/{data_name}_gt.csv',\n",
    "        \"embedding_path\": f'../tmp/embedding/{data_name}_sgcn_feature05.pkl', # tmp folder for cross-validation\n",
    "        \"regression_weights_path\": f'../tmp/weights/{data_name}_sgcn_feature05.pkl',\n",
    "        \"inductive_model_path\": None, # f'../output/inductive/{data_name}_model', # or None\n",
    "        \"log_path\": f'../logs/{data_name}_logs_feature05.json',\n",
    "        \"epochs\":300,\n",
    "        \"test_size\":0.33,\n",
    "        \"reduction_iterations\": 128,\n",
    "        \"reduction_dimensions\": 30,\n",
    "        \"seed\": 42,\n",
    "        \"lamb\": 0.0,\n",
    "        \"learning_rate\": 0.001,  \n",
    "        \"weight_decay\": 10e-4, \n",
    "        # \"layers\": [64, 32,16,8],\n",
    "        \"layers\": [32,16],\n",
    "        \"spectral_features\":False,\n",
    "        \"general_features\": True,  \n",
    "        \"sample_num\":None,\n",
    "        \"class_weights\":False,\n",
    "        \"node_under_sampling\":False,\n",
    "        \"hidden_residual\":False,\n",
    "        \"eval_freq\":1,\n",
    "        \"subgraph_training\":False,\n",
    "        \"l1_lambda\":0.1,\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class_weights</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edge_path</th>\n",
       "      <td>../data/input/alpha/alpha_network.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embedding_path</th>\n",
       "      <td>../data/tmp/embedding/alpha_sgcn_feature.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epochs</th>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>features_path</th>\n",
       "      <td>../data/input/alpha/alpha_node_feature.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>general_features</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hidden_residual</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inductive_model_path</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l1_lambda</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lamb</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layers</th>\n",
       "      <td>[32, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_path</th>\n",
       "      <td>../logs/alpha_logs_feature.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>node_under_sampling</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nodes_path</th>\n",
       "      <td>../data/input/alpha/alpha_gt.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reduction_dimensions</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reduction_iterations</th>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regression_weights_path</th>\n",
       "      <td>../data/tmp/weights/alpha_sgcn_feature.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_num</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seed</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spectral_features</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subgraph_training</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_size</th>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_decay</th>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    0\n",
       "class_weights                                                   False\n",
       "edge_path                       ../data/input/alpha/alpha_network.csv\n",
       "embedding_path           ../data/tmp/embedding/alpha_sgcn_feature.pkl\n",
       "epochs                                                            300\n",
       "eval_freq                                                           1\n",
       "features_path              ../data/input/alpha/alpha_node_feature.csv\n",
       "general_features                                                 True\n",
       "hidden_residual                                                 False\n",
       "inductive_model_path                                             None\n",
       "l1_lambda                                                         0.1\n",
       "lamb                                                                0\n",
       "layers                                                       [32, 16]\n",
       "learning_rate                                                   0.001\n",
       "log_path                              ../logs/alpha_logs_feature.json\n",
       "node_under_sampling                                             False\n",
       "nodes_path                           ../data/input/alpha/alpha_gt.csv\n",
       "reduction_dimensions                                               30\n",
       "reduction_iterations                                              128\n",
       "regression_weights_path    ../data/tmp/weights/alpha_sgcn_feature.pkl\n",
       "sample_num                                                       None\n",
       "seed                                                               42\n",
       "spectral_features                                               False\n",
       "subgraph_training                                               False\n",
       "test_size                                                        0.33\n",
       "weight_decay                                                    0.001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.io.json.json_normalize(args).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold cross-validation\n",
    "- train : validation : test = 6:3:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tab_printer(args)\n",
    "edges, nodes_dict = read_graph(args) # nodes_dict['indice']:node_id , nodes_dict['label'] : label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=0)\n",
    "all_indice = nodes_dict['indice']\n",
    "all_labels = nodes_dict['label']\n",
    "auc_scores = []\n",
    "regression_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Training Phase ====\n",
      "0-th fold\n",
      "labels:(array([-1,  1]), array([ 91, 124]))\n",
      "\n",
      "Training started.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGCN (Loss=0.0189): 100%|██████████| 300/300 [03:01<00:00,  1.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Epoch</td>\n",
       "      <td>AUC</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.825203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.842276</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.860976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.873984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.887805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.895935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.901626</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.904065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.910569</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.915447</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.917886</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.919512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.922764</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.927642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.936585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.943089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.946341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.954472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.95935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.965854</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.965041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.964228</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.965041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.965041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.968293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>271</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>272</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>273</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>274</td>\n",
       "      <td>0.978862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>275</td>\n",
       "      <td>0.978862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>276</td>\n",
       "      <td>0.978049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>277</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>278</td>\n",
       "      <td>0.978049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>279</td>\n",
       "      <td>0.978049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>280</td>\n",
       "      <td>0.978049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>281</td>\n",
       "      <td>0.978049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>282</td>\n",
       "      <td>0.978049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>283</td>\n",
       "      <td>0.978049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>284</td>\n",
       "      <td>0.978049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>285</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>286</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>287</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>288</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>289</td>\n",
       "      <td>0.978049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>290</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>291</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>292</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>293</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>294</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>295</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>296</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>300</td>\n",
       "      <td>0.977236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1   2\n",
       "0    Epoch       AUC  F1\n",
       "1        1  0.792683   0\n",
       "2        2  0.825203   0\n",
       "3        3  0.842276   0\n",
       "4        4  0.860976   0\n",
       "5        5  0.873984   0\n",
       "6        6  0.887805   0\n",
       "7        7  0.895935   0\n",
       "8        8  0.901626   0\n",
       "9        9  0.904065   0\n",
       "10      10  0.910569   0\n",
       "11      11  0.915447   0\n",
       "12      12  0.917886   0\n",
       "13      13  0.919512   0\n",
       "14      14  0.922764   0\n",
       "15      15  0.927642   0\n",
       "16      16  0.933333   0\n",
       "17      17  0.936585   0\n",
       "18      18  0.943089   0\n",
       "19      19  0.946341   0\n",
       "20      20  0.954472   0\n",
       "21      21   0.95935   0\n",
       "22      22  0.963415   0\n",
       "23      23  0.965854   0\n",
       "24      24  0.965041   0\n",
       "25      25  0.964228   0\n",
       "26      26  0.965041   0\n",
       "27      27  0.965041   0\n",
       "28      28  0.966667   0\n",
       "29      29  0.968293   0\n",
       "..     ...       ...  ..\n",
       "271    271  0.977236   0\n",
       "272    272  0.977236   0\n",
       "273    273  0.977236   0\n",
       "274    274  0.978862   0\n",
       "275    275  0.978862   0\n",
       "276    276  0.978049   0\n",
       "277    277  0.977236   0\n",
       "278    278  0.978049   0\n",
       "279    279  0.978049   0\n",
       "280    280  0.978049   0\n",
       "281    281  0.978049   0\n",
       "282    282  0.978049   0\n",
       "283    283  0.978049   0\n",
       "284    284  0.978049   0\n",
       "285    285  0.977236   0\n",
       "286    286  0.977236   0\n",
       "287    287  0.977236   0\n",
       "288    288  0.977236   0\n",
       "289    289  0.978049   0\n",
       "290    290  0.977236   0\n",
       "291    291  0.977236   0\n",
       "292    292  0.977236   0\n",
       "293    293  0.977236   0\n",
       "294    294  0.977236   0\n",
       "295    295  0.977236   0\n",
       "296    296  0.977236   0\n",
       "297    297  0.977236   0\n",
       "298    298  0.977236   0\n",
       "299    299  0.977236   0\n",
       "300    300  0.977236   0\n",
       "\n",
       "[301 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loss:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Test Phase ====\n",
      "0-th fold's auc_score:1.0\n",
      "[[14  0]\n",
      " [ 1 10]]\n",
      "\n",
      "==== Training Phase ====\n",
      "1-th fold\n",
      "labels:(array([-1,  1]), array([ 91, 124]))\n",
      "\n",
      "Training started.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SGCN (Loss=0.0362):  18%|█▊        | 55/300 [00:33<02:27,  1.66it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e1e2f5b489b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSignedGCNTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_nodes_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_and_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SDGCN/models/sgcn.py\u001b[0m in \u001b[0;36mcreate_and_train_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;31m#                 loss, _ = self.model(self.positive_edges, self.negative_edges, self.y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositive_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_indice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SGCN (Loss=%g)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(kf.split(X=nodes_dict['indice'],y=nodes_dict['label'])):\n",
    "    print(\"==== Training Phase ====\")\n",
    "    print('{}-th fold'.format(i))\n",
    "    # training\n",
    "    train_node_indice = all_indice[train_index]\n",
    "    train_node_labels = all_labels[train_index]\n",
    "    print('labels:{}'.format(np.unique(train_node_labels,return_counts=True)))\n",
    "    tmp_nodes_dict = {}\n",
    "    tmp_nodes_dict['all_ncount'] = nodes_dict['all_ncount']\n",
    "    tmp_nodes_dict['indice'] = train_node_indice\n",
    "    tmp_nodes_dict['label'] = train_node_labels\n",
    "    trainer = SignedGCNTrainer(args, edges, tmp_nodes_dict)\n",
    "    trainer.setup_dataset()\n",
    "    trainer.create_and_train_model()\n",
    "    \n",
    "    if args.test_size > 0:\n",
    "        # trainer.save_model() ## trainer.create_and_train_model()のなかで，すでにbest_modelが保存されている．\n",
    "        # score_printer(trainer.logs)\n",
    "        display(pd.DataFrame(trainer.logs['performance']))\n",
    "        save_logs(args, trainer.logs)\n",
    "\n",
    "    # test\n",
    "    print(\"==== Test Phase ====\")\n",
    "    test_node_indice = all_indice[test_index]\n",
    "    test_node_labels = all_labels[test_index]\n",
    "    # feature = pd.read_csv(args.embedding_path,index_col='id').values\n",
    "    feature = pd.read_pickle(args.embedding_path).drop('id',1).values\n",
    "    test_feature = feature[test_node_indice]\n",
    "    # weight = pd.read_csv(args.regression_weights_path)\n",
    "    weight = pd.read_pickle(args.regression_weights_path)\n",
    "    predictions = np.dot(test_feature,weight.values.T)\n",
    "    # probabilities = torch.nn.functional.softmax(torch.from_numpy(predictions)).numpy()\n",
    "    probabilities = torch.sigmoid(torch.from_numpy(predictions)).numpy()\n",
    "    predict_labels = (probabilities>=0.5).astype(int)\n",
    "    # auc_score = roc_auc_score(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_score=probabilities[:,1])\n",
    "    auc_score = roc_auc_score(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_score=probabilities)\n",
    "    auc_scores.append(auc_score)\n",
    "    cmx = confusion_matrix(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_pred=predict_labels)\n",
    "    regression_weights.append(trainer.model.regression_weights)\n",
    "    print(\"{0}-th fold's auc_score:{1}\".format(i,auc_score))\n",
    "    print(cmx)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(probabilities[test_node_labels==1],alpha=0.5,bins=10)\n",
    "_ = plt.hist(probabilities[test_node_labels==-1],alpha=0.5,bins=10)\n",
    "# plt.xlim(0,0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(trainer.model.regression_weights.cpu().detach().numpy()[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_name = input('データセット：')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "        \"edge_path\": f'../input/{data_name}/{data_name}_network.csv',\n",
    "        \"features_path\":  f'../input/{data_name}/{data_name}_node_feature.csv',\n",
    "        \"nodes_path\": f'../input/{data_name}/{data_name}_gt.csv',\n",
    "        \"embedding_path\": f'../tmp/embedding/{data_name}_sgcn_feature05.pkl', # tmp folder for cross-validation\n",
    "        \"regression_weights_path\": f'../tmp/weights/{data_name}_sgcn_feature05.pkl',\n",
    "        \"inductive_model_path\": f'../output/inductive/{data_name}_model', # or None\n",
    "        \"log_path\": f'../logs/{data_name}_logs_feature05.json',\n",
    "        \"epochs\":300,\n",
    "        \"test_size\":0.2,\n",
    "        \"reduction_iterations\": 128,\n",
    "        \"reduction_dimensions\": 30,\n",
    "        \"seed\": 42,\n",
    "        \"lamb\": 0.0,\n",
    "        \"learning_rate\": 0.001,  \n",
    "        \"weight_decay\": 10e-4, \n",
    "        # \"layers\": [64, 32,16,8],\n",
    "        \"layers\": [32, 16, ],\n",
    "        \"spectral_features\":False,\n",
    "        \"general_features\": True,  \n",
    "        \"sample_num\":None,\n",
    "        \"class_weights\":False,\n",
    "        \"node_under_sampling\":False,\n",
    "        \"hidden_residual\":False,\n",
    "        \"eval_freq\":1,\n",
    "        \"subgraph_training\":False,\n",
    "        \"l1_lambda\":0.05,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tab_printer(args)\n",
    "edges, nodes_dict = read_graph(args) # nodes_dict['indice']:node_id , nodes_dict['label'] : label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(nodes_dict['label'],return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy task (学習に全て使う)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = SignedGCNTrainer(args, edges, nodes_dict)\n",
    "trainer.setup_dataset()\n",
    "trainer.create_and_train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.test_size > 0:\n",
    "    # trainer.save_model()\n",
    "    # score_printer(trainer.logs)\n",
    "    display(pd.DataFrame(trainer.logs['performance']))\n",
    "    save_logs(args, trainer.logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(arr=trainer.used_train_indice, file=f'../output/inductive/{data_name}_train_indice.npy')\n",
    "\n",
    "# np.save(arr=trainer.used_test_indice, file=f'../output/inductive/{data_name}_test_indice.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inductive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_name = input('データセット：')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_args = easydict.EasyDict({\n",
    "        \"edge_path\": f'../input/{data_name}/{data_name}_network.csv',\n",
    "        \"features_path\":  f'../input/{data_name}/{data_name}_node_feature.csv',\n",
    "        \"nodes_path\": f'../input/{data_name}/{data_name}_gt.csv',\n",
    "        \"embedding_path\": f'../tmp/embedding/{data_name}_sgcn_feature05.pkl', # tmp folder for cross-validation\n",
    "        \"regression_weights_path\": f'../tmp/weights/{data_name}_sgcn_feature05.pkl',\n",
    "        \"inductive_model_path\": f'../output/inductive/{data_name}_model', # or None\n",
    "        \"log_path\": f'../logs/{data_name}_logs_feature05.json',\n",
    "        \"epochs\":300,\n",
    "        \"test_size\":0.33,\n",
    "        \"reduction_iterations\": 128,\n",
    "        \"reduction_dimensions\": 30,\n",
    "        \"seed\": 42,\n",
    "        \"lamb\": 0.0,\n",
    "        \"learning_rate\": 0.001,  \n",
    "        \"weight_decay\": 10e-4, \n",
    "        # \"layers\": [64, 32,16,8],\n",
    "        \"layers\": [32, 16, ],\n",
    "        \"spectral_features\":False,\n",
    "        \"general_features\": True,  \n",
    "        \"sample_num\":None,\n",
    "        \"class_weights\":False,\n",
    "        \"node_under_sampling\":False,\n",
    "        \"hidden_residual\":False,\n",
    "        \"eval_freq\":1,\n",
    "        \"subgraph_training\":False,\n",
    "        \"l1_lambda\":0.0,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_edges, new_nodes_dict = read_graph(new_args)\n",
    "\n",
    "X = np.array(pd.read_csv(f'../input/{data_name}/{data_name}_node_feature.csv')) # general node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_dataset = input('学習に使ったデータセット：')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = SignedGCNPredictor(new_args, f'../output/inductive/{training_dataset}_model', X, new_edges,new_nodes_dict)\n",
    "\n",
    "predictions = predictor.predict()\n",
    "predict_labels = predictions.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_node_raw = np.load(f'../input/{training_dataset}/{training_dataset}_label_encoder.npy')\n",
    "\n",
    "newly_added_node_judger = ~np.isin(new_nodes_dict['indice'],trained_node_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = new_nodes_dict['label'][newly_added_node_judger]\n",
    "y_score_indice = new_nodes_dict['indice'][newly_added_node_judger]\n",
    "\n",
    "roc_auc_score(y_true=[1 if i==-1 else 0 for i in y_true],y_score=predictions[y_score_indice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "confusion_matrix([1 if i==-1 else 0 for i in new_nodes_dict['label']],predict_labels[new_nodes_dict['indice']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(predictions[new_nodes_dict['indice']][y_true==1],alpha=0.5,bins=10)\n",
    "_ = plt.hist(predictions[new_nodes_dict['indice']][y_true==-1],alpha=0.5,bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## epinions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'epinions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "        \"edge_path\": f'../input/{data_name}/{data_name}_network.csv',#'../input/{data_name}/user_network.csv',\n",
    "        \"features_path\":  f'../input/{data_name}/{data_name}_node_feature.csv',#'../input/{data_name}/user_network.csv',\n",
    "        \"nodes_path\": f'../input/{data_name}/{data_name}_gt.csv',\n",
    "        \"embedding_path\": f'../tmp/embedding/{data_name}_sgcn_feature05.pkl', # tmp folder for cross-validation\n",
    "        \"regression_weights_path\": f'../tmp/weights/{data_name}_sgcn_feature05.pkl',\n",
    "        \"inductive_model_path\": f'../output/inductive/{data_name}_model', # or None\n",
    "        \"log_path\": f'../logs/{data_name}_logs_feature05.json',\n",
    "        \"epochs\":150,\n",
    "        \"test_size\":0.33,\n",
    "        \"reduction_iterations\": 128,\n",
    "        \"reduction_dimensions\": 30,\n",
    "        \"seed\": 42,\n",
    "        \"lamb\": 0.0,\n",
    "        \"learning_rate\": 0.001,  \n",
    "        \"weight_decay\": 10e-4, \n",
    "        # \"layers\": [64, 32,16,8],\n",
    "        \"layers\": [32, 16, ],\n",
    "        \"spectral_features\":False,\n",
    "        \"general_features\": True,  \n",
    "        \"sample_num\":None,\n",
    "        \"class_weights\":False,\n",
    "        \"node_under_sampling\":False,\n",
    "        \"hidden_residual\":False,\n",
    "        \"eval_freq\":1,\n",
    "        \"subgraph_training\":True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(pd.io.json.json_normalize(args).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tab_printer(args)\n",
    "edges, nodes_dict = read_graph(args) # nodes_dict['indice']:node_id , nodes_dict['label'] : label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array(pd.read_csv(args.features_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph_making(args,edges,nodes_dict,sample_node_num,train_node_indice_original,neighbor_sampling=True):\n",
    "    original_network_df = pd.read_csv(args.edge_path)\n",
    "    if neighbor_sampling == True:\n",
    "        first_neightbors = np.unique(original_network_df.loc[(original_network_df.id1_.isin(train_node_indice_original)) | \n",
    "                                                                                                      (original_network_df.id2_.isin(train_node_indice_original)), ['id1_','id2_']].values)\n",
    "        sampled_node_indice = np.random.choice(first_neightbors,sample_node_num,replace=False)\n",
    "    else:\n",
    "        sampled_node_indice = set(np.random.choice(np.arange(nodes_dict['all_ncount']), sample_node_num, replace=False)) | set(train_node_indice_original)\n",
    "\n",
    "    sub_network_df = \\\n",
    "        original_network_df.loc[(original_network_df.id1_.isin(sampled_node_indice)) & (original_network_df.id2_.isin(sampled_node_indice))]\n",
    "\n",
    "    original_gt_df = pd.read_csv(args.nodes_path)\n",
    "\n",
    "    sub_gt_df = \\\n",
    "        original_gt_df.copy().loc[(original_gt_df.node_id.isin(sub_network_df.id1_)) & (original_gt_df.node_id.isin(sub_network_df.id2_))]\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    subGraph_map_encoder = LabelEncoder()\n",
    "\n",
    "    subGraph_map_encoder.fit(list(set(sub_network_df.id1_) | set(sub_network_df.id2_) | set(sub_gt_df.node_id)))\n",
    "\n",
    "    sub_gt_df['node_id'] = subGraph_map_encoder.transform(sub_gt_df.node_id)\n",
    "\n",
    "    sub_network_df['id1_'] = subGraph_map_encoder.transform(sub_network_df.id1_)\n",
    "\n",
    "    sub_network_df['id2_'] = subGraph_map_encoder.transform(sub_network_df.id2_)\n",
    "\n",
    "    sub_edges = {}\n",
    "\n",
    "    sub_edges['positive_edges'] = sub_network_df.loc[sub_network_df.weight==1,['id1_','id2_']].values.tolist()\n",
    "\n",
    "    sub_edges['negative_edges'] = sub_network_df.loc[sub_network_df.weight==-1,['id1_','id2_']].values.tolist()\n",
    "\n",
    "    sub_edges['ecount'] = len(sub_network_df)\n",
    "\n",
    "    sub_edges['ncount'] = len(set(sub_network_df['id1_']) | set(sub_network_df['id2_']))\n",
    "\n",
    "    sub_nodes_dict = {}\n",
    "\n",
    "    sub_nodes_dict['indice'] = sub_gt_df.node_id.values\n",
    "    sub_nodes_dict['label'] = sub_gt_df.label.values\n",
    "\n",
    "    sub_nodes_dict['all_ncount'] = len(set(sub_network_df.id1_) | set(sub_network_df.id2_) | set(sub_gt_df.node_id))\n",
    "\n",
    "    sub_nodes_dict['subgraph_map_from_original_feature'] = subGraph_map_encoder.classes_    \n",
    "    return sub_edges, sub_nodes_dict, subGraph_map_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits=10,shuffle=True, random_state=0)\n",
    "all_indice = nodes_dict['indice']\n",
    "all_labels = nodes_dict['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auc_scores = []\n",
    "# train : 90 % cross-validation\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X=nodes_dict['indice'],y=nodes_dict['label'])):\n",
    "    print(\"==== Training Phase ====\")\n",
    "    print(f'{i}-th fold')\n",
    "    # training\n",
    "    train_node_indice_original = all_indice[train_index] #⬅️これは元々のグラフのnode_indice\n",
    "\n",
    "    # extract sub-graph\n",
    "    sub_edges, sub_nodes_dict,subGraph_map_encoder = subgraph_making(args,edges,nodes_dict,10000, train_node_indice_original)\n",
    "                                                                     # np.random.choice(train_node_indice_original,1000))\n",
    "    \n",
    "    train_indice_boolean_judger = np.isin(subGraph_map_encoder.inverse_transform(sub_nodes_dict['indice']),train_node_indice_original)\n",
    "    train_node_indice = sub_nodes_dict['indice'][train_indice_boolean_judger]\n",
    "    train_node_labels = sub_nodes_dict['label'][train_indice_boolean_judger]\n",
    "    print(f'labels:{np.unique(train_node_labels,return_counts=True)}')\n",
    "    tmp_nodes_dict = {}\n",
    "    tmp_nodes_dict['all_ncount'] = sub_nodes_dict['all_ncount']\n",
    "    tmp_nodes_dict['indice'] = train_node_indice\n",
    "    tmp_nodes_dict['label'] = train_node_labels\n",
    "    tmp_nodes_dict['subgraph_map_from_original_feature'] = subGraph_map_encoder.classes_\n",
    "    trainer = SignedGCNTrainer(args, sub_edges, tmp_nodes_dict)\n",
    "    trainer.setup_dataset()\n",
    "    trainer.create_and_train_model()\n",
    "    \n",
    "    if args.test_size > 0:\n",
    "        # trainer.save_model() ## trainer.create_and_train_model()のなかで，すでにbest_modelが保存されている．\n",
    "        # score_printer(trainer.logs)\n",
    "        display(pd.DataFrame(trainer.logs['performance']))\n",
    "        save_logs(args, trainer.logs)\n",
    "\n",
    "    # test\n",
    "    print(\"==== Test Phase ====\")\n",
    "    test_node_indice = all_indice[test_index]\n",
    "    test_node_labels = all_labels[test_index]\n",
    "    predictor = SignedGCNPredictor(args, args.inductive_model_path, test_X, edges,nodes_dict)\n",
    "    predictions = predictor.predict()\n",
    "    predict_labels = predictions.argmax(1)\n",
    "    auc_score = roc_auc_score(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_score=predictions[:,1][test_node_indice])\n",
    "    auc_scores.append(auc_score)\n",
    "    cmx = confusion_matrix(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_pred=predict_labels[test_node_indice])\n",
    "    print(f\"{i}-th fold's auc_score:{auc_score}\")\n",
    "    print(cmx)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## epinions Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indice = nodes_dict['indice']\n",
    "all_labels = nodes_dict['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustness_experiments(training_rates_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
    "    all_auc_scores = []\n",
    "    done_train_rate = []\n",
    "    training_rates = training_rates_list\n",
    "    for train_rate in training_rates:\n",
    "        print(f'train_rate : {train_rate}')\n",
    "        auc_scores = []\n",
    "        for i in range(10): \n",
    "            train_index, test_index = train_test_split(np.arange(len(nodes_dict['indice'])),\n",
    "                                                       stratify=nodes_dict['label'],train_size=float(train_rate),shuffle=True)\n",
    "\n",
    "            print(\"==== Training Phase ====\")\n",
    "            print(f'{i}-th')\n",
    "            # training\n",
    "            train_node_indice_original = all_indice[train_index] #⬅️これは元々のグラフのnode_indice\n",
    "            # extract sub-graph\n",
    "            sub_edges, sub_nodes_dict,subGraph_map_encoder = subgraph_making(args,edges,nodes_dict, 20000, \n",
    "                                                                             np.random.choice(train_node_indice_original,1000))\n",
    "            train_indice_boolean_judger = np.isin(subGraph_map_encoder.inverse_transform(sub_nodes_dict['indice']),train_node_indice_original)\n",
    "            train_node_indice = sub_nodes_dict['indice'][train_indice_boolean_judger]\n",
    "            train_node_labels = sub_nodes_dict['label'][train_indice_boolean_judger]\n",
    "\n",
    "            print(f'labels:{np.unique(train_node_labels,return_counts=True)}')\n",
    "            tmp_nodes_dict = {}\n",
    "            tmp_nodes_dict['all_ncount'] = sub_nodes_dict['all_ncount']\n",
    "            tmp_nodes_dict['indice'] = train_node_indice\n",
    "            tmp_nodes_dict['label'] = train_node_labels\n",
    "            tmp_nodes_dict['subgraph_map_from_original_feature'] = subGraph_map_encoder.classes_\n",
    "            trainer = SignedGCNTrainer(args, sub_edges, tmp_nodes_dict)\n",
    "            trainer.setup_dataset()\n",
    "            trainer.create_and_train_model()\n",
    " \n",
    "            if args.test_size > 0:\n",
    "                # trainer.save_model() ## trainer.create_and_train_model()のなかで，すでにbest_modelが保存されている．\n",
    "                # score_printer(trainer.logs)\n",
    "                display(pd.DataFrame(trainer.logs['performance']))\n",
    "                save_logs(args, trainer.logs)\n",
    "\n",
    "            # test\n",
    "            print(\"==== Test Phase ====\")\n",
    "            test_node_indice = all_indice[test_index]\n",
    "            test_node_labels = all_labels[test_index]\n",
    "            predictor = SignedGCNPredictor(args, args.inductive_model_path, test_X, edges,nodes_dict)\n",
    "            predictions = predictor.predict()\n",
    "            predict_labels = predictions.argmax(1)\n",
    "            auc_score = roc_auc_score(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_score=predictions[:,1][test_node_indice])\n",
    "            auc_scores.append(auc_score)\n",
    "            cmx = confusion_matrix(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_pred=predict_labels[test_node_indice])\n",
    "\n",
    "            print(f\"{i}-th fold's auc_score:{auc_score}\")\n",
    "            print(cmx)\n",
    "            print()\n",
    "        all_auc_scores.append(np.mean(auc_scores))\n",
    "        done_train_rate.append(train_rate)\n",
    "        print(f'rate{train_rate} --> {np.mean(auc_scores)}')\n",
    "        pd.DataFrame(all_auc_scores,index=done_train_rate).to_csv('tmp_{data_name}.csv'.format(data_name))\n",
    "    return pd.DataFrame(all_auc_scores, index=training_rates, columns=['average_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epi_result_df = robustness_experiments([0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_result_df.to_csv('epinions_robustness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_result_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロバストネスの検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = input('データセット：')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "        \"edge_path\": f'../input/{data_name}/{data_name}_network.csv',\n",
    "        \"features_path\":  f'../input/{data_name}/{data_name}_node_feature.csv',\n",
    "        \"nodes_path\": f'../input/{data_name}/{data_name}_gt.csv',\n",
    "        \"embedding_path\": f'../tmp/embedding/{data_name}_sgcn_feature05.pkl', # tmp folder for cross-validation\n",
    "        \"regression_weights_path\": f'../tmp/weights/{data_name}_sgcn_feature05.pkl',\n",
    "        \"inductive_model_path\": None, # f'../output/inductive/{data_name}_model', # or None\n",
    "        \"log_path\": f'../logs/{data_name}_logs_feature05.json',\n",
    "        \"epochs\":100,\n",
    "        \"test_size\":0.33,\n",
    "        \"reduction_iterations\": 128,\n",
    "        \"reduction_dimensions\": 30,\n",
    "        \"seed\": 42,\n",
    "        \"lamb\": 0.0,\n",
    "        \"learning_rate\": 0.001,  \n",
    "        \"weight_decay\": 10e-4, \n",
    "        # \"layers\": [64, 32,16,8],\n",
    "        \"layers\": [32,16,],\n",
    "        \"spectral_features\":False,\n",
    "        \"general_features\": True,  \n",
    "        \"sample_num\":None,\n",
    "        \"class_weights\":False,\n",
    "        \"node_under_sampling\":False,\n",
    "        \"hidden_residual\":False,\n",
    "        \"eval_freq\":1,\n",
    "        \"subgraph_training\":False,\n",
    "        \"l1_lambda\":0.05,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tab_printer(args)\n",
    "edges, nodes_dict = read_graph(args) # nodes_dict['indice']:node_id , nodes_dict['label'] : label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indice = nodes_dict['indice']\n",
    "all_labels = nodes_dict['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustness_experiments(training_rates_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
    "    all_auc_scores = []\n",
    "    done_train_rate = []\n",
    "    training_rates = training_rates_list\n",
    "    for train_rate in training_rates:\n",
    "        auc_scores = []\n",
    "        for i in range(30): \n",
    "            train_index, test_index = train_test_split(np.arange(len(nodes_dict['indice'])),\n",
    "                                                       stratify=nodes_dict['label'],train_size=float(train_rate),shuffle=True)\n",
    "            print(\"==== Training Phase ====\")\n",
    "            print(f'{i}-th')\n",
    "            # training\n",
    "            train_node_indice = all_indice[train_index]\n",
    "            train_node_labels = all_labels[train_index]\n",
    "            print(f'labels:{np.unique(train_node_labels,return_counts=True)}')\n",
    "            tmp_nodes_dict = {}\n",
    "            tmp_nodes_dict['all_ncount'] = nodes_dict['all_ncount']\n",
    "            tmp_nodes_dict['indice'] = train_node_indice\n",
    "            tmp_nodes_dict['label'] = train_node_labels\n",
    "            trainer = SignedGCNTrainer(args, edges, tmp_nodes_dict)\n",
    "            trainer.setup_dataset()\n",
    "            trainer.create_and_train_model()\n",
    "\n",
    "            if args.test_size > 0:\n",
    "                # trainer.save_model() ## trainer.create_and_train_model()のなかで，すでにbest_modelが保存されている．\n",
    "                # score_printer(trainer.logs)\n",
    "                display(pd.DataFrame(trainer.logs['performance']))\n",
    "                save_logs(args, trainer.logs)\n",
    "\n",
    "            # test\n",
    "            print(\"==== Test Phase ====\")\n",
    "            test_node_indice = all_indice[test_index]\n",
    "            test_node_labels = all_labels[test_index]\n",
    "            # feature = pd.read_csv(args.embedding_path,index_col='id').values\n",
    "            feature = pd.read_pickle(args.embedding_path).drop('id',1).values\n",
    "            test_feature = feature[test_node_indice]\n",
    "            # weight = pd.read_csv(args.regression_weights_path)\n",
    "            weight = pd.read_pickle(args.regression_weights_path)\n",
    "            predictions = np.dot(test_feature,weight.values.T)\n",
    "            probabilities = torch.nn.functional.softmax(torch.from_numpy(predictions)).numpy()\n",
    "            predict_labels = probabilities.argmax(1)\n",
    "            auc_score = roc_auc_score(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_score=probabilities[:,1])\n",
    "            auc_scores.append(auc_score)\n",
    "            cmx = confusion_matrix(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_pred=predict_labels)\n",
    "            print(f\"{i}-th fold's auc_score:{auc_score}\")\n",
    "            print(cmx)\n",
    "            print()\n",
    "        print(f\"train_rate : {train_rate} --> {np.mean(auc_scores)}\")\n",
    "        done_train_rate.append(train_rate)\n",
    "        all_auc_scores.append(np.mean(auc_scores))\n",
    "        pd.DataFrame(all_auc_scores,index=done_train_rate).to_csv('../logs/{}_tmp'.format(data_name))\n",
    "    return pd.DataFrame(all_auc_scores, index=training_rates, columns=['average_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#result_df = robustness_experiments([0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.03])\n",
    "result_df = robustness_experiments([0.03,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.to_csv('amazon_robustness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rate = input('トレーニングデータの比率：')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auc_scores = []\n",
    "for i in range(10): # trainとtestを逆にする\n",
    "    train_index, test_index = train_test_split(np.arange(len(nodes_dict['indice'])),stratify=nodes_dict['label'],train_size=float(train_rate)\n",
    "                                               ,shuffle=True)\n",
    "    print(\"==== Training Phase ====\")\n",
    "    print(f'{i}-th')\n",
    "    # training\n",
    "    train_node_indice = all_indice[train_index]\n",
    "    train_node_labels = all_labels[train_index]\n",
    "    print(f'labels:{np.unique(train_node_labels,return_counts=True)}')\n",
    "    tmp_nodes_dict = {}\n",
    "    tmp_nodes_dict['all_ncount'] = nodes_dict['all_ncount']\n",
    "    tmp_nodes_dict['indice'] = train_node_indice\n",
    "    tmp_nodes_dict['label'] = train_node_labels\n",
    "    trainer = SignedGCNTrainer(args, edges, tmp_nodes_dict)\n",
    "    trainer.setup_dataset()\n",
    "    trainer.create_and_train_model()\n",
    "    \n",
    "    if args.test_size > 0:\n",
    "        # trainer.save_model() ## trainer.create_and_train_model()のなかで，すでにbest_modelが保存されている．\n",
    "        # score_printer(trainer.logs)\n",
    "        display(pd.DataFrame(trainer.logs['performance']))\n",
    "        save_logs(args, trainer.logs)\n",
    "\n",
    "    # test\n",
    "    print(\"==== Test Phase ====\")\n",
    "    test_node_indice = all_indice[test_index]\n",
    "    test_node_labels = all_labels[test_index]\n",
    "    # feature = pd.read_csv(args.embedding_path,index_col='id').values\n",
    "    feature = pd.read_pickle(args.embedding_path).drop('id',1).values\n",
    "    test_feature = feature[test_node_indice]\n",
    "    # weight = pd.read_csv(args.regression_weights_path)\n",
    "    weight = pd.read_pickle(args.regression_weights_path)\n",
    "    predictions = np.dot(test_feature,weight.values.T)\n",
    "    probabilities = torch.nn.functional.softmax(torch.from_numpy(predictions)).numpy()\n",
    "    predict_labels = probabilities.argmax(1)\n",
    "    auc_score = roc_auc_score(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_score=probabilities[:,1])\n",
    "    auc_scores.append(auc_score)\n",
    "    cmx = confusion_matrix(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_pred=predict_labels)\n",
    "    print(f\"{i}-th fold's auc_score:{auc_score}\")\n",
    "    print(cmx)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
